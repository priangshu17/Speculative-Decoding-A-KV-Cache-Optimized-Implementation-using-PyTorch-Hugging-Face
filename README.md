# Speculative Decoding â€” A KV-Cache Optimized Implementation (PyTorch + Hugging Face)

This repository contains a clean, modular, and research-oriented implementation of **Speculative Decoding** for encoderâ€“decoder transformers.  
The project includes:

- A **baseline autoregressive decoder** (Mp)
- A **KV-cache optimized speculative decoding algorithm** using a smaller proposal model (Mq)
- Proper **acceptanceâ€“rejection logic** as described in *Fast Inference from Transformers via Speculative Decoding*
- **Speedup benchmarking**, **JS divergence measurement**, **acceptance rate (Î±)**, and **per-token Mp distribution tracking**
- A full **experiment harness** + **plots** to visualize how Î³ (gamma) affects performance and fidelity

This implementation is optimized to run efficiently on **consumer GPUs** (e.g., RTX 4050 6GB), with memory-efficient KV-cache decoding and single-token forward passes.

---

## ğŸš€ Highlights

- **Supports encoderâ€“decoder architectures** (T5, BART, etc.)
- Uses **past_key_values** to ensure *constant memory per decoding step*
- Modular file layout (no monolithic scripts)
- Experiment runner generates:
  - Speedup vs Î³ plot  
  - Î± (acceptance rate) vs Î³ plot  
  - JS divergence vs Î³ plot  
  - CSV logs for reproducibility  

The codebase is designed for learning, experimentation, and potential extension into research projects.

---

## ğŸ“‚ Project Structure

IMPLEMENTING_SPECULATIVE_DECODING/
â”‚
â”œâ”€â”€ specdec/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ utils.py
â”‚ â”œâ”€â”€ sampling.py
â”‚ â”œâ”€â”€ metrics.py
â”‚ â”‚
â”‚ â”œâ”€â”€ baseline/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â””â”€â”€ encoder_decoder_baseline.py
â”‚ â”‚
â”‚ â”œâ”€â”€ speculative/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ encoder_decoder.py # KV-cache optimized speculative decoding
â”‚ â””â”€â”€ batching.py # helpers (kept minimal)
â”‚
â”œâ”€â”€ experiments/
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ run_experiment.py # full benchmarking harness
â”‚
â”œâ”€â”€ pyproject.toml # installable package config
â”œâ”€â”€ README.md
â””â”€â”€ results/ # saved results & plots


---

## ğŸ“˜ Background: What Is Speculative Decoding?

Speculative Decoding accelerates transformer inference by:

1. Using a **fast approximator model (Mq)** to *guess* Î³ future tokens.
2. Using the **large target model (Mp)** to **validate** or **reject** these guesses.
3. If guesses look good, they are accepted *without* calling Mp for every single token.
4. When a guess fails, Mp samples the next token itself.

This allows:

- **Fewer Mp forward passes**
- **Near-identical output quality**
- **Significant speedups** (especially when Mp â‰« Mq)

---

## ğŸ“ Acceptance Rule (Short Summary)

A guess token *xi* from Mq is accepted if:
u â‰¤ p(xi) / q(xi)

Where:
- `p(xi)` = Mpâ€™s probability  
- `q(xi)` = Mqâ€™s probability  
- `u ~ Uniform(0,1)`  

If rejected:
- We fall back to sampling from **p0(x)** = max(0, p(x) âˆ’ q(x)) normalized.

This implementation faithfully follows the original paper.

---

## âš¡ KV-Cache Optimization (Why It Matters)

Traditional speculative decoding recomputes large padded batches.  
This implementation avoids that by:

- Calling Mp and Mq with **only one new token at each step**
- Reusing **past_key_values**
- Keeping GPU memory extremely low
- Making speculative decoding viable on **6GB GPUs**

This is a *production-style* speculative decoder.

---

## ğŸ“Š Metrics & Plots

Running the experiment script generates:

### **1. Speedup vs Gamma**
Shows how speculative decoding compares to baseline autoregressive decoding.

### **2. Acceptance Rate Î± vs Gamma**
Measures proposal quality from Mq.

### **3. JS Divergence vs Gamma**
Measures divergence between Mp-only outputs and speculative outputs.

Plots + CSV logs are saved automatically.

---

## â–¶ï¸ Running the Code

### 1. Install the package locally
```bash
pip install -e .
```

### 2. Run the Experiments harness
python experiments/run_experiment.py

### 3. View Results
Plots and logs appear under:
results_kvcache/
    speedup.png
    alpha.png
    js.png
    results.csv
    


















